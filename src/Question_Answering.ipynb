{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "from transformers import DefaultDataCollator\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open('../Data/citation_data.json')\n",
    "lines = data_file.readlines()\n",
    "json_lines = [json.loads(line) for line in lines]\n",
    "cc_df = pd.DataFrame(json_lines)\n",
    "len(cc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADD NEW QA Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df['context'] = cc_df['text_before_explicit_citation'].str.cat([cc_df['explicit_citation'], cc_df['text_after_explicit_citation']], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>human_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For example, using German as a pivot, they ext...</td>\n",
       "      <td>Other important publications about pivoting ap...</td>\n",
       "      <td>Other important publications about pivoting ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One relevant line of research is on machine tr...</td>\n",
       "      <td>This has been tried for a number of language p...</td>\n",
       "      <td>This has been tried for a number of language p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They use a variety of approaches #OTHEREFR), m...</td>\n",
       "      <td>It is also worth noting that there has been so...</td>\n",
       "      <td>It is also worth noting that there has been so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Many researchers have investigated the use of ...</td>\n",
       "      <td>Pivoting has been explored for closely related...</td>\n",
       "      <td>Pivoting has been explored for closely related...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moreover, Al-Sabbagh and Girju #OTHEREFR descr...</td>\n",
       "      <td>#REFR argued that for very close languages, e....</td>\n",
       "      <td>#REFR argued that for very close languages, e....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  For example, using German as a pivot, they ext...   \n",
       "1  One relevant line of research is on machine tr...   \n",
       "2  They use a variety of approaches #OTHEREFR), m...   \n",
       "3  Many researchers have investigated the use of ...   \n",
       "4  Moreover, Al-Sabbagh and Girju #OTHEREFR descr...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Other important publications about pivoting ap...   \n",
       "1  This has been tried for a number of language p...   \n",
       "2  It is also worth noting that there has been so...   \n",
       "3  Pivoting has been explored for closely related...   \n",
       "4  #REFR argued that for very close languages, e....   \n",
       "\n",
       "                                         answer_text  human_labeled  \n",
       "0  Other important publications about pivoting ap...              0  \n",
       "1  This has been tried for a number of language p...              0  \n",
       "2  It is also worth noting that there has been so...              0  \n",
       "3  Pivoting has been explored for closely related...              0  \n",
       "4  #REFR argued that for very close languages, e....              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = cc_df[['context','explicit_citation', 'implicit_citation_0.9', \"human_labeled\"]]\n",
    "dataframe = dataframe.rename(columns={ 'explicit_citation': 'question', 'implicit_citation_0.9':'answer_text'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_hl_df = dataframe[dataframe['human_labeled'] == 1]  # Replace 'column_name' with the name of the column you want to filter ondf.to_json('output.json', orient='records')  # Replace 'output.json' with the desired file name and path\n",
    "cc_df = dataframe[dataframe['human_labeled'] == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (68041, 4)\n",
      "Test set shape: (9505, 4)\n",
      "Validation set shape: (9506, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have a dataframe called 'df'\n",
    "df = cc_df.sample(frac = 1, random_state=42)\n",
    "\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Splitting the test set into test and validation sets\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "#Transfer from val to test\n",
    "rows = val_df[:1000]\n",
    "val_df[1000:]\n",
    "test_df = pd.concat([test_df, rows])\n",
    "\n",
    "#Add Human Labelled\n",
    "val_df  = pd.concat([val_df, cc_hl_df])\n",
    "\n",
    "# # Optionally, you can reset the index of the dataframes\n",
    "# train_df.reset_index(drop=True, inplace=True)\n",
    "# test_df.reset_index(drop=True, inplace=True)\n",
    "# val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Printing the shapes of the datasets to verify the split\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get start position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_and_end(row):\n",
    "    paragraph = row['context']\n",
    "    context = row['answer_text']\n",
    "    \n",
    "    start_index = int(paragraph.find(context))\n",
    "    end_index = int(start_index + len(context) - 1)\n",
    "    return pd.Series([start_index, end_index])\n",
    "    \n",
    "train_df[['start_index', 'end_index']] = train_df.apply(get_start_and_end, axis=1)\n",
    "test_df[['start_index', 'end_index']] = test_df.apply(get_start_and_end, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"roberta-large\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "max_length = 512\n",
    "doc_stride = 128\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_answers(r):\n",
    "    start = r[0]\n",
    "    text = r[1]\n",
    "    return {\n",
    "        'answer_start': [start],\n",
    "        'text': [text]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['answers'] = train_df[['start_index', 'answer_text']].apply(convert_answers, axis=1)\n",
    "test_df['answers'] = test_df[['start_index', 'answer_text']].apply(convert_answers, axis=1)\n",
    "#df_train = dataframe[:-64].reset_index(drop=True)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75129db39a77446e9ed3731df3776c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f3610123f74cb59577b7cfdf699158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9505 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_ds = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test_ds = test_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define optimiser and collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_DISABLED=True\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"qa_model\"\n",
    "learning_rate = 2e-5\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "num_train_epochs = 3\n",
    "weight_decay = 0.01\n",
    "data_collator = DefaultDataCollator()\n",
    "%env WANDB_DISABLED=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_dataloader = DataLoader(tokenized_train_ds, batch_size=train_batch_size, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_test_ds, batch_size=eval_batch_size , collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Optimiser and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,  # You can adjust this value\n",
    "    num_training_steps=len(train_dataloader) * num_train_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Training):   0%|          | 0/4253 [01:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_train_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} (Training)\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'batch_loss': loss.item(), 'total_loss': total_loss / (step + 1)})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {total_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "# Save model if needed\n",
    "model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running to model on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger #REFR, which was trained on the respective CoNLL training data (the files train.conll).\n",
      "Answer: The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger #REFR, which was trained on the respective CoNLL training data (the files train.conll).  The quality of such tagging is not very high, since we do not use any lexicons11 or pretrained models. However, it is sufficient for obtaining good reducibility scores. 8We do not have appropriate Chinese segmenter that would segment Chinese texts in the same way as in CoNLL.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForQuestionAnswering,  RobertaTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load custom-trained QA model and tokenizer\n",
    "model_path = \"qa_model\"  # Update with the path to your custom model\n",
    "tokenizer_path = \"roberta-large\"  # Update with the path to your custom tokenizer\n",
    "model = RobertaForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Example input question and context\n",
    "question = \"The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger #REFR, which was trained on the respective CoNLL training data (the files train.conll).\"\n",
    "context = \"For obtaining reducibility scores, we used the W2C corpus9 of Wikipedia articles, which was downloaded by Majlis? and Z?abokrtsky? #OTHEREFR. Their statistics across languages are shown in Table 4. To make them useful, the necessary preprocessing steps must have been done.The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger #REFR, which was trained on the respective CoNLL training data (the files train.conll).  The quality of such tagging is not very high, since we do not use any lexicons11 or pretrained models. However, it is sufficient for obtaining good reducibility scores. 8We do not have appropriate Chinese segmenter that would segment Chinese texts in the same way as in CoNLL.\"\n",
    "\n",
    "# Tokenize and encode the input\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "mod = model(**inputs)\n",
    "start_logits = mod['start_logits']\n",
    "end_logits = mod[\"end_logits\"]\n",
    "\n",
    "\n",
    "# Decode the predictions\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits) + 1\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index])\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
